#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ---------------------------------------------------------------------------
# Authors: John G Samuelson <johnsam@mit.edu>
#          Christoph Dinh <christoph.dinh@brain-link.de>
# Created: May, 2020
# License: MIT
# ---------------------------------------------------------------------------

import numpy as np
from scipy.optimize import minimize, basinhopping
import matplotlib.pyplot as plt
from plyfile import PlyData, PlyElement
import nibabel as nib
import pickle
import os
import evaler
from scipy import signal
from .helpers import *

def get_cerebellum_data(cmb_path):
    """
    Checks if the required cerebellum data are available and download if not.
    
    Parameters
    ----------
    cmb_path : str
        Path to the ceremegbellum folder.
    """
    if os.path.exists(cmb_path + 'data/cerebellum_geo') and \
        os.path.isdir(os.environ['RESULTS_FOLDER'] + '/nnUNet/3d_fullres/Task001_mask') and \
        os.path.isdir(os.environ['RESULTS_FOLDER'] + '/nnUNet/3d_fullres/Task002_lh') and \
        os.path.isdir(os.environ['RESULTS_FOLDER'] + '/nnUNet/3d_fullres/Task003_rh') and \
        os.path.isdir(os.environ['RESULTS_FOLDER'] + '/nnUNet/3d_fullres/Task004_refine_lobsI_IV') and \
        os.path.exists(cmb_path + 'data/brain.nii')    :
            print('The required atlas data and segmentation models seem to be downloaded.')
    else:
        from pooch import retrieve
        import zipfile
        print('Seems like some data are missing. No problem, fetching...')
        os.system('mkdir ' + cmb_path + 'tmp')
        retrieve(url='https://osf.io/x5ryb/download',
                 known_hash=None, fname='ceremegbellum',
                 path=cmb_path + 'tmp') # UNTIL THE REPO IS PUBLIC, YOU NEED TO DO THIS STEP MANUALLY
        with zipfile.ZipFile(cmb_path + 'tmp/' + 'ceremegbellum.zip', 'r') as zip_ref:
            zip_ref.extractall(cmb_path + 'tmp')
        os.system('mv ' + cmb_path + 'tmp/ceremegbellum/cerebellum_geo ' + cmb_path + \
                  'data/cerebellum_geo')
        os.system('mv ' + cmb_path + 'tmp/ceremegbellum/brain.nii ' + cmb_path + \
                  'data/brain.nii')
        os.system('mv ' + cmb_path + 'tmp/ceremegbellum/Task* ' + os.environ['RESULTS_FOLDER'] + \
                  '/nnUNet/3d_fullres/')
        os.system('rm -r ' + cmb_path + 'tmp') # clean up
        print('Done.')
    return

<<<<<<< HEAD
def change_labels(vol, old_labels, new_labels):
    new_vol = vol.copy()
    for c, old_label in enumerate(old_labels):
        mask_inds = np.where(vol == old_label)
        new_vol[mask_inds[0], mask_inds[1], mask_inds[2]] = new_labels[c]
    return new_vol

def save_nifti_from_3darray(vol, fname, rotate=False, affine=None):
    if rotate:
        vol = vol[:, ::-1, ::-1]
        vol = np.transpose(vol, axes=(0, 2, 1))
    mgz = nib.Nifti1Image(vol, affine=affine)
    nib.save(mgz, fname)
    print('saved to '+fname)
    return mgz
=======
>>>>>>> e811129f3e2984f64fe39234dca353de1905fce1

def parse_patch(filename, **kargs):
    import struct
    with open(filename, 'rb') as fp:
        header, = struct.unpack('>i', fp.read(4))
        nverts, = struct.unpack('>i', fp.read(4))
        data = np.fromstring(fp.read(), dtype=[('vert', '>i4'), ('x', '>f4'), ('y', '>f4'), ('z', '>f4')])
        assert len(data) == nverts
        return data


def cartesian2vols(rr, scale, center):
    rr = rr * scale
    rr = rr + (center - np.mean(rr, axis=0))
    return rr


def transform_to_vol_space(rr_high, cx_subj_vols):
    bound_box_vol = (np.max(cx_subj_vols[:, 0]) - np.min(cx_subj_vols[:, 0])) \
                    * (np.max(cx_subj_vols[:, 1]) - np.min(cx_subj_vols[:, 1])) \
                    * (np.max(cx_subj_vols[:, 2]) - np.min(cx_subj_vols[:, 2]))
    bound_box_vol_high = (np.max(rr_high[:, 0]) - np.min(rr_high[:, 0])) \
                         * (np.max(rr_high[:, 1]) - np.min(rr_high[:, 1])) \
                         * (np.max(rr_high[:, 2]) - np.min(rr_high[:, 2]))
    center = np.mean(cx_subj_vols, axis=0)
    rr_0 = cartesian2vols(rr_high, (bound_box_vol / bound_box_vol_high)**(1/3), 
                                   center)
    return rr_0


def contrast_fit(cx_subj_vols, cerb_wm_vols, high_vols_cx, high_vols_wm, weights):
    # Create fake contrasts in rgb, then weight by weights arg in cost function
    subj_vols = np.zeros((256, 256, 256, 3))
    subj_vols[:, :, :, :] = np.array([0, 0, 1])    
    subj_vols[cx_subj_vols[:, 0], cx_subj_vols[:, 1], cx_subj_vols[:, 2], :] = np.array([1, 0, 0])
    subj_vols[cerb_wm_vols[:, 0], cerb_wm_vols[:, 1], cerb_wm_vols[:, 2], :] = np.array([0, 1, 0])

    c_0 = np.array([1.])
    r_0 = np.random.rand(3)
    R_0 = 0.1*np.random.rand(3)
    res_nonlinear = basinhopping(cost_contrast, np.hstack((np.hstack((c_0, r_0)), R_0)), 
                                 minimizer_kwargs = {'args' : (weights,
                                                               high_vols_cx, 
                                                               high_vols_wm, 
                                                               subj_vols)})
    para = res_nonlinear['x']
    rr_fitted = np.rint(solid_body_transform(para[0:3], para[3:6], 
                                                       high_vols_cx)).astype(int)
    return rr_fitted, res_nonlinear


def vol2int(vol, max_vol):
    max_vol = max_vol*2 # Multiply by 2 to handle negative integers
    return max_vol**0+vol[0]+max_vol**1*vol[1]+max_vol**2*vol[2]


def convert_to_lia_coords(vol, aseg, hemi, crop_pad):

    if hemi=='lh':
        origo = np.min(np.array(np.where(np.isin(aseg, [7, 8]))).T, axis=0)
    if hemi=='rh':
        origo = np.min(np.array(np.where(np.isin(aseg, [46, 47]))).T, axis=0)
    
    foreground_coords = np.array(np.nonzero(vol)).T
    lia_coords = foreground_coords + origo - crop_pad

    vol_lia_frame = np.zeros((256, 256, 256))
    vol_lia_frame[lia_coords[:, 0], lia_coords[:, 1], lia_coords[:, 2]] = vol[np.nonzero(vol)]
    
    return vol_lia_frame


<<<<<<< HEAD
def split_cerebellar_hemis(subjects_dir, subject, output_folder):
    aseg_nib = nib.load(subjects_dir+subject+'/mri/aseg.mgz')
    aseg = np.asanyarray(aseg_nib.dataobj)
    image = nib.load(subjects_dir+subject+'/mri/orig.mgz')
    image_vol = np.asanyarray(image.dataobj)
    mask_vol = np.asanyarray(nib.load((output_folder+'/mask/output/cerebellum_001.nii.gz')).dataobj)
    pads = ((np.array(aseg.shape)-np.array(mask_vol.shape))/2).astype(int)
    mask_aligned = np.zeros((256, 256, 256))
    mask_aligned[pads[0]:256-pads[0], pads[1]:256-pads[1], pads[2]:256-pads[2]] = mask_vol

    mask = np.array(np.nonzero(mask_aligned)).T
    lh = np.where(np.isin(aseg, [7, 8]))
    rh = np.where(np.isin(aseg, [46, 47]))
    lh_rh_vol = np.zeros(aseg.shape).astype(int)
    lh_rh_vol[lh] = 1
    lh_rh_vol[rh] = 2
    aseg_cerb = np.concatenate((np.array(lh).T, np.array(rh).T), axis=0)
    all_voxels = np.unique(np.concatenate((mask, aseg_cerb), axis=0), axis=0)
    aseg_ints = np.dot(aseg_cerb, np.array([1, 256, 256**2]))
    mask_ints = np.dot(mask, np.array([1, 256, 256**2]))
    unsigned_voxels = mask[~(np.isin(mask_ints, aseg_ints))]
    neighbors = np.array([[[[x, y, z] for x in np.arange(-1, 2)] for y in np.arange(-1, 2)] for z in np.arange(-1, 2)]).reshape(27,3)
    
    while len(unsigned_voxels)>0:
        assigned = np.zeros(len(unsigned_voxels))
        type_vals = []
        for c, vox in enumerate(unsigned_voxels):
            all_neighbors = neighbors+vox
            all_neighbors = all_neighbors[np.concatenate((all_neighbors < np.array(lh_rh_vol.shape), all_neighbors > np.array([-1, -1, -1])), axis=1).all(axis=1)]
            val_neighbors = lh_rh_vol[all_neighbors[:, 0], all_neighbors[:, 1], all_neighbors[:, 2]]
            val_neighbors = val_neighbors[~(val_neighbors == 0)] # Remove background
            if len(val_neighbors) > 0:
                counts = np.bincount(val_neighbors)
                type_val = np.argmax(counts)
                type_vals.append(type_val)
                assigned[c] = 1
        vox_to_assign = unsigned_voxels[np.nonzero(assigned)]
        lh_rh_vol[vox_to_assign[:,0], vox_to_assign[:,1], vox_to_assign[:,2]] = type_vals
        unsigned_voxels = unsigned_voxels[np.where(assigned==0)]

    lh_split = np.zeros((256, 256, 256))
    lh_split[np.where(lh_rh_vol == 1)] = image_vol[np.where(lh_rh_vol == 1)]
    rh_split = np.zeros((256, 256, 256))
    rh_split[np.where(lh_rh_vol == 2)] = image_vol[np.where(lh_rh_vol == 2)]
    mask = np.zeros((256, 256, 256)).astype(int)
    mask[np.where(lh_rh_vol == 2)] = 1
    mask[np.where(lh_rh_vol == 1)] = 1

    save_nifti_from_3darray(mask, output_folder+'/../'+subject+'_mask.nii.gz', rotate=False, affine=image.affine)
    save_nifti_from_3darray(lh_split, output_folder+'/lh/cerebellum_001_0000.nii.gz', rotate=False, affine=image.affine)
    save_nifti_from_3darray(rh_split, output_folder+'/rh/cerebellum_001_0000.nii.gz', rotate=False, affine=image.affine)
    
    return (mask, lh_split, rh_split)


=======
>>>>>>> e811129f3e2984f64fe39234dca353de1905fce1
def volumetric_segmentation(rr, cx_subj_vols, scale_factor=1):
    # Scale up to make sure no part of the cerebellum is "closed"
    rr_0 = transform_to_vol_space(rr, cx_subj_vols)
    rr_0 = affine_transform(1, np.array([0,0,0]), [np.pi/2, 0, 0], rr_0)
    cortex_vols = np.unique(np.rint(rr_0*scale_factor), axis=0).astype(int)
    max_vol = np.max(cortex_vols)+1
    start_vol = (np.mean(cortex_vols, axis=0)+[20*scale_factor,0,0]).astype(int)
    front_line_vols = start_vol.reshape((1,3))
    saved_vols = start_vol.reshape((1,3))
    black_vols = [vol2int(vol, max_vol) for vol in cortex_vols]
    black_vols.append(vol2int(start_vol, max_vol))
    delta = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], 
                      [0, 0, 1], [0, 0, -1]])
    
    mins = np.min(cortex_vols,axis=0)-1
    maxs = np.max(cortex_vols,axis=0)+1
    X, Y, Z = np.mgrid[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]] 
    all_voxels = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T
    f_vox2int = {}
    for c, vox in enumerate(all_voxels):
        ind = vol2int(vox, max_vol=max_vol)
        f_vox2int.update({tuple(vox) : ind})

    while len(front_line_vols) > 0:
        for vol in front_line_vols:
            neighbors = vol + delta
            for neighbor in neighbors:
#                if not vol2int(neighbor, max_vol) in black_vols:
                if not f_vox2int[tuple(neighbor)] in black_vols:
                    front_line_vols = np.vstack((front_line_vols, neighbor))
                    saved_vols = np.vstack((saved_vols, neighbor))
#                    black_vols.append(vol2int(neighbor, max_vol))
                    black_vols.append(f_vox2int[tuple(neighbor)])
            front_line_vols = front_line_vols[1: front_line_vols.shape[0], :]
        print(len(front_line_vols))
        
    # Scale back down and return mixed voxels and enclosed voxels
    saved_vols_org = np.unique(np.rint(saved_vols/scale_factor).astype(int),axis=0)
    cortex_vols_org = np.unique(np.rint(cortex_vols/scale_factor).astype(int), axis=0)
    enclosed_vols = []
    mixed_vols = []
    cx_ints = []
    for cx_vol in cortex_vols_org:
        cx_ints.append(vol2int(cx_vol, max_vol))
    cx_ints = np.array(cx_ints).astype(int)
    for vol in saved_vols_org:
        if vol2int(vol, max_vol) in cx_ints:
            mixed_vols.append(vol)
        else:
            enclosed_vols.append(vol)
    
    return np.array(enclosed_vols), np.array(mixed_vols), cortex_vols_org

def get_average_normals(nn, rr_vol, plot=False):
    if nn.shape[0] != rr_vol.shape[0]:
        raise Exception('Number of normals must be the same as number of voxels.')
    vols_unique, inds, counts = np.unique(rr_vol, axis=0, return_inverse=True,
                                          return_counts=True)
    mapp = []
    for ind in range(len(vols_unique)):
        mapp.append(np.where(ind == inds)[0])
    
    nn_ave = []
    for c, vol in enumerate(vols_unique):
        nn_ave.append(np.mean(nn[mapp[c], :], axis=0))
    
    if plot:
        norms = np.linalg.norm(np.array(nn_ave), axis=1)
        plt.figure('cumulative')
        y = plt.hist(norms, bins=100, cumulative=True)
        plt.close('cumulative')
        plt.plot(y[1][1:len(y[1])],y[0]/len(norms))
        plt.xlabel('Norm of vector average')
        plt.ylabel('Voxels (% cumulative)')
    
    return np.array(nn_ave)

def print_mgz(mgz, orig_vols, rr_vols, contrasts, data_dir, subject):
    """
    Print mgz giving rr_fitted voxels 0 contrast in the vol data.
    """
    if len(rr_vols) != len(contrasts):
        raise Exception('rr_vols and contrasts must be same length')
    vol_mod = mgz.get_data().copy()
    vol_mod[orig_vols[:, 0], orig_vols[:, 1], orig_vols[:, 2]] = 0
    for c, rr_vol_set in enumerate(rr_vols):
        vol_mod[rr_vol_set[:, 0], rr_vol_set[:, 1], rr_vol_set[:, 2]] = contrasts[c]
    mgz_mod = nib.Nifti1Image(vol_mod, mgz.affine, mgz.header, 
                              mgz.extra, mgz.file_map)

    nib.save(mgz_mod, data_dir + subject + '_tf.mgz')
    print('volume data saved to ' + data_dir + subject + '_tf.mgz')
    return mgz_mod


def mean_cont2tissue_cont(hr_vol, subj_data, cx_subj_vols, wm_subj_vols):
    cx_contrast = np.mean(subj_data[cx_subj_vols[:, 0], cx_subj_vols[:, 1], cx_subj_vols[:, 2]])
    wm_contrast = np.mean(subj_data[wm_subj_vols[:, 0], wm_subj_vols[:, 1], wm_subj_vols[:, 2]])
    tissue_contrast = 2*cx_contrast*hr_vol*np.heaviside(0.5-hr_vol, 0.5) + \
                       (2*(wm_contrast-cx_contrast)*hr_vol + 2*cx_contrast-wm_contrast) \
                       *np.heaviside(hr_vol-0.5, 0.5)
    return tissue_contrast

def hr_int2mean_cont(hr_vol_int):
    unit = 1/np.max(hr_vol_int)
    hr_vol = hr_vol_int*unit    
    return hr_vol

def space_grid(mins, maxs, steps):
    X, Y, Z = np.meshgrid(np.linspace(mins[0], maxs[0], num=steps[0]), 
                          np.linspace(mins[1], maxs[1], num=steps[1]), 
                          np.linspace(mins[2], maxs[2], num=steps[2]))
    return np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T

def blurring_vol(vol, blurring_steps=3):
    hr_vol_blurred = vol
    voxels_coo = space_grid(mins=(1, 1, 1), maxs=vol.shape-np.array((2, 2, 2)), steps=vol.shape-np.array((2, 2, 2))).astype(int)
    neighbor_mesh = space_grid([-1, -1, -1], [1, 1, 1], steps=[3, 3, 3]).astype(int) 
    for step in range(blurring_steps):
        hr_vol_blurred_static = hr_vol_blurred.copy()
        print('\n \n blurring step ' + str(step) + '\n ')
        for c, vox in enumerate(voxels_coo):
            neighbors = neighbor_mesh + vox
            mean_contrast = np.mean([hr_vol_blurred_static[neighbor[0], neighbor[1], neighbor[2]] for neighbor in neighbors])
            hr_vol_blurred[vox[0], vox[1], vox[2]] = mean_contrast
            print(str(c/len(voxels_coo)*100)[0:3]+' % complete         \r',end='')
    return hr_vol_blurred

def extract_cerb(vol_data, cx_vols, wm_vols, pad=9):
    x = range(np.min(cx_vols[:, 0])-pad, np.max(cx_vols[:, 0])+pad)
    y = range(np.min(cx_vols[:, 1])-pad, np.max(cx_vols[:, 1])+pad)
    z = range(np.min(cx_vols[:, 2])-pad, np.max(cx_vols[:, 2])+pad)
    subj = np.zeros(vol_data.shape)
    subj[cx_vols[:, 0], cx_vols[:, 1], cx_vols[:, 2]] = \
        vol_data[cx_vols[:, 0], cx_vols[:, 1], cx_vols[:, 2]]
    subj[wm_vols[:, 0], wm_vols[:, 1], wm_vols[:, 2]] = \
        vol_data[wm_vols[:, 0], wm_vols[:, 1], wm_vols[:, 2]]
    subj = subj[x, :, :][:, y, :][:, :, z]
    
    return subj

def blurring(hr_rs, plot=False):
    """
    Blurs a volume by convolution with a Gaussian kernel.
    """

    # Create the kernel
    sigma = 3.0     # width of kernel
    x = np.arange(-5,6,1)   # coordinate arrays -- make sure they contain 0!
    y = np.arange(-5,6,1)
    z = np.arange(-5,6,1)
    xx, yy, zz = np.meshgrid(x,y,z)
    kernel = np.exp(-(xx**2 + yy**2 + zz**2)/(2*sigma**2))
    kernel = kernel/np.sum(kernel)  # Normalize kernel so that homogenous regions stay the same
    
    # apply to sample data
    high_res_filtered = signal.convolve(hr_rs, kernel, mode="same")
    high_res_filtered[np.where(hr_rs > 5)] = hr_rs[np.where(hr_rs > 5)]
    high_res_filtered = signal.convolve(high_res_filtered, kernel, mode="same")
    high_res_filtered[np.where(hr_rs > 5)] = hr_rs[np.where(hr_rs > 5)]
    high_res_filtered = signal.convolve(high_res_filtered, kernel, mode="same")
    high_res_filtered[np.where(hr_rs > 5)] = hr_rs[np.where(hr_rs > 5)]
    
    sigma = 1.0     # width of kernel
    x = np.arange(-3,4,1)   # coordinate arrays -- make sure they contain 0!
    y = np.arange(-3,4,1)
    z = np.arange(-3,4,1)
    xx, yy, zz = np.meshgrid(x,y,z)
    kernel = np.exp(-(xx**2 + yy**2 + zz**2)/(2*sigma**2))
    kernel = kernel/np.sum(kernel)  # Normalize kernel so that homogenous regions stay the same
    
    high_res_filtered = signal.convolve(high_res_filtered, kernel, mode="same", method='fft')
    if plot:
        fig, ax = plot_sagittal(high_res_filtered, title='High-res (blurred)')
        
    return high_res_filtered


def image_filter(vols, filter_type, threshold=None, plot=False):
    from scipy import signal

    # Create the kernel
    sigma = 2.0     # width of kernel
    x = np.arange(-5,6,1)   # coordinate arrays -- make sure they contain 0!
    y = np.arange(-5,6,1)
    z = np.arange(-5,6,1)
    xx, yy, zz = np.meshgrid(x,y,z)
    kernel = np.exp(-(xx**2 + yy**2 + zz**2)/(2*sigma**2))
    if filter_type == 'highpass':
        kernel[5, 5, 5] = - (np.sum(kernel) - 1)
    kernel = kernel/np.sum(np.abs(kernel))  # Normalize kernel so that homogenous regions stay the same
    
    # apply to sample data
    vols_hp = signal.convolve(vols, kernel, mode="same", method='fft')    
        
    # Threshold and clean
    if not threshold is None:
        vols_hpt = np.zeros(vols_hp.shape)
        vols_hpt[np.where(vols_hp > threshold)] = 1.0
    
    if plot:
        plot_sagittal(vols_hpt, title='Warped points in subj vol') 
    
    return vols_hpt


def get_convex_hull_2d(points):
    import scipy
    cx_hull = scipy.spatial.ConvexHull(points)
    hull_points = points[cx_hull.vertices, :]
    hull_points = np.concatenate((hull_points, hull_points[0,:].reshape(1,2)), axis=0)
    return hull_points
    
<<<<<<< HEAD

def plot_cerebellum_data(data, fwd_src, org_src, cerebellum_geo, cort_data=None, flatmap_cmap='bwr', mayavi_cmap=None,
                         smoothing_steps=0, view='all', sub_sampling='sparse', cmap_lims=[1,98]):
    """Plots data on the cerebellar cortical surface. Requires cerebellum geometry file
    to be downloaded.
    
    Parameters
    ----------
    data : array, shape (n_vertices)
        Cerebellar data
    rr : array, shape (n_vertices, 3)
        Positions of subject-specific vertices.
    cmap : str
        Colormap. Needs to be avaible in both mayavi and plotly.
    view: "all" | "normal" | "inflated" | "flatmap"
        Which views to show. If view='all', then all (normal, inflated and flamap) are shown.
        
    Returns
    -------
    figures: list
        List containing Figure objects.
    
    """

    from mayavi import mlab
    import matplotlib.colors as colors
    import matplotlib.tri as mtri
    from evaler import print_surf
    
    if not cort_data is None:
        assert cort_data.shape[0]==fwd_src[0]['nuse'], 'cort_data and src[0][\'nuse\'] must have the same number of elements.'
    
    def truncate_colormap(flatmap_cmap, minval=0.0, maxval=1.0, n=500):
        new_cmap = colors.LinearSegmentedColormap.from_list(
            'trunc({n},{a:.2f},{b:.2f})'.format(n=flatmap_cmap.name, a=minval, b=maxval),
            flatmap_cmap(np.linspace(minval, maxval, n)))
        return new_cmap

    figures = []
    src_cerb = fwd_src[1]
    src_cort = fwd_src[0]
    
    print('Smoothing...')
    estimate_smoothed = np.zeros(cerebellum_geo['dw_data'][sub_sampling+'_verts'].shape[0])
    estimate_smoothed[:] = np.nan
    estimate_smoothed[src_cerb['vertno']] = data
    nan_verts = np.where(np.isnan(estimate_smoothed))[0]

    while len(nan_verts) > 0:
        vert_neighbors = np.array(cerebellum_geo['dw_data'][sub_sampling+'_vert_to_neighbor'])[nan_verts]
        estimate_smoothed[nan_verts] = [np.nanmean(estimate_smoothed[vert_neighbor_group]) for vert_neighbor_group in vert_neighbors]
        nan_verts = np.where(np.isnan(estimate_smoothed))[0]

    if not cort_data is None:
        if not org_src[0]['use_tris'] is None:
            cort_full_mantle = np.zeros(org_src[0]['nuse'])
            cort_full_mantle[:] = np.nan
            cort_full_mantle[np.isin(org_src[0]['vertno'], src_cort['vertno'])] = cort_data
            nan_verts = np.where(np.isnan(cort_full_mantle))[0]
            vert_inuse = np.zeros(src_cort['np']).astype(int)
            vert_inuse[org_src[0]['vertno']] = range(org_src[0]['nuse'])
            tris_frame = vert_inuse[org_src[0]['use_tris']]
        else:
            print('use_tris is None, so we have to spread estimates over entire cortical source space...')
            cort_full_mantle = np.zeros(org_src[0]['np'])
            cort_full_mantle[src_cort['vertno']] = cort_data
            nan_verts = np.where(np.isnan(cort_full_mantle))[0]
            tris_frame = org_src[0]['tris']
#            cort_full_mantle[:] = np.nan
#        while len(nan_verts) > 0:
#            vert2tris = np.array([np.where(np.isin(tris_frame, vert).any(axis=1)) for vert in nan_verts])
#            neighbors = [np.unique(tris_frame[x[0]]) for x in vert2tris]
#            cort_full_mantle[nan_verts] = [np.nanmean(cort_full_mantle[neighbor_group]) for neighbor_group in neighbors]
#            nan_verts = np.where(np.isnan(cort_full_mantle))[0]
#            print('Remaining source points: '+str(len(nan_verts)))

    if mayavi_cmap is None:
        if cort_data is None:
            if np.min(estimate_smoothed) < 0:
                mayavi_cmap = 'bwr'
            else:
                mayavi_cmap = 'OrRd'
        else:
            if np.min(np.concatenate((estimate_smoothed, cort_data))) < 0:
                mayavi_cmap = 'bwr'
            else:
                mayavi_cmap = 'OrRd'


#    estimate_smoothed[np.where(np.isin(src_cerb_org['vertno'], src_cerb['vertno']))] = data
    for step in range(smoothing_steps):
        print('Step '+str(step))
        for vert in range(estimate_smoothed.shape[0]):
            estimate_smoothed[vert] = np.nanmean(estimate_smoothed[cerebellum_geo['dw_data'][sub_sampling+'_vert_to_neighbor'][vert]])

    if view in ['all', 'normal']:
#        print_surf('/autofs/cluster/fusion/john/projects/cerebellum/inv/data/cerebellum_estimate.ply',
#                  src_cerb['rr'], src_cerb['tris'], cmap=mayavi_cmap, scals=estimate_smoothed, color=np.array([True]))
                 
         mlab.figure(bgcolor=(1., 1., 1.), fgcolor=(0., 0., 0.), size=(1200,1200))
         normal_fig = mlab.triangular_mesh(src_cerb['rr'][:, 0], src_cerb['rr'][:, 1], src_cerb['rr'][:, 2],
                                           cerebellum_geo['dw_data'][sub_sampling+'_tris'], scalars=estimate_smoothed, colormap=mayavi_cmap)
         mlab.colorbar()
         figures.append(normal_fig)
         if not cort_data is None:
             if not org_src[0]['use_tris'] is None:
                 rr_cx = src_cort['rr'][org_src[0]['vertno'], :]
             else:
                 rr_cx = src_cort['rr']
             normal_fig = mlab.triangular_mesh(rr_cx[:, 0], rr_cx[:, 1], rr_cx[:, 2],
                                              tris_frame, scalars=cort_full_mantle, colormap=mayavi_cmap)
             figures.append(normal_fig)
             
#            print_surf('/autofs/cluster/fusion/john/projects/cerebellum/inv/data/cort_estimate.ply',
#                      src_cort['rr'][org_src[0]['vertno']], tris_frame, cmap=mayavi_cmap, scals=cort_full_mantle, color=np.array([True]))
             
    if view in ['all', 'inflated']:
        # print_surf('/autofs/cluster/fusion/john/projects/cerebellum/inv/data/cerebellum_estimate_inflated.ply',
        #            cerebellum_geo['verts_inflated'][cerebellum_geo['dw_data'][sub_sampling],:],
        #            cerebellum_geo['dw_data'][sub_sampling+'_tris'], cmap=mayavi_cmap,
        #            scals=estimate_smoothed, color=np.array([True]))
        mlab.figure(bgcolor=(1., 1., 1.), fgcolor=(0., 0., 0.), size=(1200,1200))
        inflated_fig = mlab.triangular_mesh(cerebellum_geo['verts_inflated_fs'][cerebellum_geo['dw_data'][sub_sampling], 0],
                                            cerebellum_geo['verts_inflated_fs'][cerebellum_geo['dw_data'][sub_sampling], 1],
                                            cerebellum_geo['verts_inflated_fs'][cerebellum_geo['dw_data'][sub_sampling], 2], 
                                            cerebellum_geo['dw_data'][sub_sampling+'_tris'], scalars=estimate_smoothed, colormap=mayavi_cmap)
        mlab.colorbar()
        figures.append(inflated_fig)
    
    if view in ['all', 'flatmap']:

        if np.min(estimate_smoothed) >= 0:
            red_cmap = truncate_colormap(plt.get_cmap(flatmap_cmap), 0.5, 1.)
            color_levels = np.ones((cmap_lims[0]+1, 4))
            color_levels = np.vstack((color_levels, red_cmap(np.linspace(0, 1, cmap_lims[1]-cmap_lims[0]))))
            color_levels = np.vstack((color_levels, np.repeat(red_cmap([1.]).reshape(1,4), repeats=100-cmap_lims[1], axis=0)))
            cmap_real=red_cmap
        else:
            blue_cmap = truncate_colormap(plt.get_cmap(flatmap_cmap), 0.0, 0.5)
            red_cmap = truncate_colormap(plt.get_cmap(flatmap_cmap), 0.5, 1.)
            color_levels = np.repeat(blue_cmap([0.]).reshape(1,4), repeats=100-cmap_lims[1], axis=0)
            color_levels = np.vstack((color_levels, blue_cmap(np.linspace(0, 1, cmap_lims[1]-cmap_lims[0]))))
            color_levels = np.vstack((color_levels, np.ones((cmap_lims[0]+1, 4))))
            color_levels = np.vstack((color_levels, np.ones((cmap_lims[0], 4))))
            color_levels = np.vstack((color_levels, red_cmap(np.linspace(0, 1, cmap_lims[1]-cmap_lims[0]))))
            color_levels = np.vstack((color_levels, np.repeat(red_cmap([1.]).reshape(1,4), repeats=100-cmap_lims[1], axis=0)))
            
        max_abs = np.max(np.abs(estimate_smoothed))
        if np.min(estimate_smoothed) >= 0:
            levels = np.linspace(0, max_abs, 101)
        else:
            levels = np.linspace(-max_abs, max_abs, 201)

        font = {'weight' : 'normal', 'size'   : 8}
        plt.rc('font', **font)
        flat_fig = plt.figure(dpi = 300, figsize = (7, 5.5))

        for flatmap in cerebellum_geo['flatmap_outlines']:
            lin = plt.plot(-flatmap[:, 0], flatmap[:, 1], linestyle='--', linewidth=0.4, c='k', alpha=1.0)[0] # minus x-coord for keeping in neurological coordinates

        for key in list(cerebellum_geo['flatmap_inds'].keys()):
            dw_inds = np.where(np.isin(cerebellum_geo['dw_data'][sub_sampling], cerebellum_geo['flatmap_inds'][key]))[0]
            dw_flatinds = cerebellum_geo['dw_data'][sub_sampling][dw_inds]
            flat_verts = cerebellum_geo['verts_flatmap'][dw_flatinds, :]
#            flat_verts = cerebellum_geo['verts_flatmap'][cerebellum_geo['flatmap_inds'][key], :]

            ind_map = np.zeros(cerebellum_geo['dw_data'][sub_sampling].shape[0])
            ind_map[:] = np.nan
            ind_map[dw_inds] = np.linspace(0, len(dw_inds)-1, len(dw_inds)).astype(int)
            tris_flat = cerebellum_geo['dw_data'][sub_sampling+'_tris'][np.where(np.isin(cerebellum_geo['dw_data'][sub_sampling+'_tris'],
                                                                                  dw_inds).all(axis=1))[0], :]
            tris_flat = ind_map[tris_flat].astype(int)
            estimate_flat_all = estimate_smoothed[dw_inds]
#            ind_map = np.zeros(cerebellum_geo['verts_inflated'].shape[0])
#            ind_map[:] = np.nan
#            ind_map[cerebellum_geo['flatmap_inds'][key]] = np.linspace(0,len(cerebellum_geo['flatmap_inds'][key])-1,
#                                                       len(cerebellum_geo['flatmap_inds'][key])).astype(int)
#            tris_flat = cerebellum_geo['faces'][np.where(np.isin(cerebellum_geo['faces'],
#                                       cerebellum_geo['flatmap_inds'][key]).all(axis=1))[0], :]
#            tris_flat = ind_map[tris_flat].astype(int)
#            estimate_flat_all = all_estimate_smoothed[cerebellum_geo['flatmap_inds'][key]]
            triang = mtri.Triangulation(-flat_verts[:,0], flat_verts[:,1], tris_flat) # minus x-coord for keeping in neurological coordinates
            # fig = plt.figure()
            # from mpl_toolkits.mplot3d import Axes3D
            # triconf = fig.add_subplot(111, projection='3d')
            # triconf.plot_trisurf(triang, estimate_flat_all)
#            plt.pcolormesh(flat_verts[:,0], flat_verts[:,1], estimate_flat_all)
            triconf = lin.axes.tricontourf(triang, estimate_flat_all, colors=color_levels, levels=levels)# flatmap_cmap=hot_truncated_cmap) 


        if np.min(levels) < 0:
            cbar = flat_fig.colorbar(triconf, ticks=[levels[0], levels[100-cmap_lims[1]], levels[100-cmap_lims[0]],
                                                     0, levels[100+cmap_lims[0]], levels[100+cmap_lims[1]], levels[len(levels)-1]])
            min_lev = str(levels[0])[0:4]+str(levels[0])[str(levels[0]).find('e'):]
            min_sat = str(levels[100-cmap_lims[1]])[0:4]+str(levels[100-cmap_lims[1]])[str(levels[100-cmap_lims[1]]).find('e'):]
            min_thresh = str(levels[100-cmap_lims[0]])[0:4]+str(levels[100-cmap_lims[0]])[str(levels[100-cmap_lims[0]]).find('e'):]
            max_lev = str(levels[len(levels)-1])[0:4]+str(levels[len(levels)-1])[str(levels[len(levels)-1]).find('e'):]
            max_sat = str(levels[100+cmap_lims[1]])[0:4]+str(levels[100+cmap_lims[1]])[str(levels[100+cmap_lims[1]]).find('e'):]
            max_thresh = str(levels[100+cmap_lims[0]])[0:4]+str(levels[100+cmap_lims[0]])[str(levels[100+cmap_lims[0]]).find('e'):]
            cbar.ax.set_yticklabels([min_lev, min_sat, min_thresh, '0', max_thresh, max_sat, max_lev])  
        else:
            cbar = flat_fig.colorbar(triconf, ticks=[0, levels[cmap_lims[0]], 
                                                     levels[cmap_lims[1]], levels[len(levels)-1]])
            max_lev = str(levels[len(levels)-1])[0:4]+str(levels[len(levels)-1])[str(levels[len(levels)-1]).find('e'):]
            max_sat = str(levels[cmap_lims[1]])[0:4]+str(levels[cmap_lims[1]])[str(levels[cmap_lims[1]]).find('e'):]
            max_thresh = str(levels[cmap_lims[0]])[0:4]+str(levels[cmap_lims[0]])[str(levels[cmap_lims[0]]).find('e'):]
            cbar.ax.set_yticklabels(['0', max_thresh, max_sat, max_lev])  

        ant_lob = np.array([[-110, 918], [-86,925], [-52, 935], [-16, 942],
                            [23, 965], [57, 980], [78, 983], [123, 966]])
        crusII_left = np.array([[-165, 257], [-126, 260], [-80, 300]])
        crusII_right = np.array([[96, 313], [230, 148]])
        lobVIIb_left = np.array([[-239, -49], [-178, -117]])
        lobVIIb_right = np.array([[255, -211], [244, -119], [265, -75], [293, -71]])
        
        for border_line in [ant_lob, crusII_left, crusII_right, lobVIIb_left, lobVIIb_right]:
            plt.plot(-border_line[:,0], border_line[:,1], linestyle='--', linewidth=0.4, c='k', alpha=1.0) # minus x-coord for keeping in neurological coordinates
        plt.gca().set_aspect('equal')

        # place text boxes outlining anatomical landmarks
        textstr = ' Lobules I-V \n (anterior lobe)'
        fontsize = 8
        flat_fig.axes[0].text(-610, 1175, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = 'Lobule VI'
        flat_fig.axes[0].text(-490, 840, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = 'Crus I'
        flat_fig.axes[0].text(-450, 441, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = ' Crus II/\n Lobule VIIb'
        flat_fig.axes[0].text(-700, 100, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = 'Lobule VIII'
        flat_fig.axes[0].text(-740, -200, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = ' Lobule IX \n (tonsil)'
        flat_fig.axes[0].text(-670, -590, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = ' Lobule X \n (flocculus)'
        flat_fig.axes[0].text(-370, -670, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = 'Inferior vermis'
        flat_fig.axes[0].text(40, -710, textstr, fontsize=fontsize,
                verticalalignment='top')
        textstr = 'Left'
        flat_fig.axes[0].text(-380, 1480, textstr, fontsize=fontsize,
                verticalalignment='top', fontweight='bold')
        textstr = 'Right'
        flat_fig.axes[0].text(200, 1480, textstr, fontsize=fontsize,
                verticalalignment='top', fontweight='bold')
        flat_fig.axes[0].arrow(-350, -580, 82, 64, head_width=20, head_length=20, linewidth=0.5, fc='k', ec='k')
        flat_fig.axes[0].arrow(-230, -660, 80, 45, head_width=20, head_length=20, linewidth=0.5, fc='k', ec='k')
        flat_fig.axes[0].arrow(20, -750, 0, 130, head_width=20, head_length=20, linewidth=0.5, fc='k', ec='k')
        flat_fig.patch.set_visible(False)
        flat_fig.axes[0].axis('off')
        plt.show()
        figures.append(flat_fig)

    return figures


def setup_cerebellum_source_space(subjects_dir, subject, cmb_path, cerebellum_subsampling='sparse',
                                  calc_nn=True, print_fs=False, plot=False, mirror=False,
                                  post_process=False, debug_mode=False):
    """Sets up the cerebellar surface source space. Requires cerebellum geometry file
    to be downloaded.
    
    Parameters
    ----------
    subjects_dir : str
        Subjects directory.
    subject : str
        Subject name.
    cerb_dir : str
        Path to cerebellum folder.
    calc_nn: Boolean
        If True, it will calculate the normals of the cerebellum source space.
    print_fs : Boolean
        If True, it will print an fs file of the cerebellar source space that can be viewed with e.g. freeview.
    plot : Boolean
        If True, will plot sagittal cross-sectional plots of the cerebellar source space suposed on subject MR data.
        
    Returns
    -------
    subj_cerb: dictionary
        Dictionary containing geometry data: vertex positions (rr), faces (tris) and normals (nn, if calc_nn is True).
    
    """
    
    from scipy import signal
    import ants
    import pandas as pd
    import evaler

    subjects_dir = subjects_dir + '/'
    print('starting subject '+subject+'...')
    # Load data
    subj_cerb = {}
    data_dir = cmb_path + 'data/'    
    cb_data = pickle.load(open(data_dir+'cerebellum_geo','rb'))
    if cerebellum_subsampling == 'full':
        rr = cb_data['verts_normal']
        tris = cb_data['faces']
    else:
        rr = cb_data['dw_data'][cerebellum_subsampling+'_verts']
        tris = cb_data['dw_data'][cerebellum_subsampling+'_tris']
        rr = affine_transform(1, np.array([0,0,0]), [np.pi/2, 0, 0], rr)

    hr_vol = cb_data['hr_vol']
    hr_segm = cb_data['parcellation']['volume'].copy()
    old_labels = [12,  33,  36,  43,  46,  53,  56,  60,  63,  66,  70,  73, 74,  75,
                  76,  77,  78,  80,  83,  84,  86,  87,  90,  93,  96, 100, 103, 106]
    hr_segm = change_labels(hr_segm, old_labels=old_labels, new_labels=np.arange(29)[1:])
    
    # Get subject segmentation
    print('Doing segmentation...')
    subj_segm = np.asanyarray(get_segmentation(subjects_dir, subject, cmb_path,
                                               post_process=post_process, debug_mode=debug_mode).dataobj)
#    subj_mask = np.asanyarray(nib.load(data_dir+'segm_folder/tmp/registered/mask_divide/'+subject+'_mask_lh_rh.nii.gz').dataobj)
    subj = np.asanyarray(nib.load(subjects_dir+subject+'/mri/orig.mgz').dataobj)

    # Mask cerebellum
    pad = 3
    cerb_coords = np.nonzero(subj_segm)
    cb_range = [[np.min(np.nonzero(subj_segm)[x])-pad for x in range(3)],
                 [np.max(np.nonzero(subj_segm)[x])+pad for x in range(3)]]
    subj_segm = subj_segm[cb_range[0][0] : cb_range[1][0],
                              cb_range[0][1] : cb_range[1][1],
                              cb_range[0][2] : cb_range[1][2]]
    subj_contrast = np.zeros(subj.shape)
    subj_contrast[cerb_coords] = subj[cerb_coords]
    subj_contrast = subj_contrast[cb_range[0][0] : cb_range[1][0],
                                  cb_range[0][1] : cb_range[1][1],
                                  cb_range[0][2] : cb_range[1][2]]

    print('Setting up adaptation to subject... ', end='', flush=True)
    hr_vol_scaled = hr_vol
    for axis in range(0,3):
        hr_vol_scaled = signal.resample(hr_vol_scaled, num=subj_segm.shape[axis], axis=axis)
    scf = np.array(hr_vol_scaled.shape) / np.array(hr_vol.shape)
    for x in range(3): rr[:, x] = rr[:, x] * scf[x]
    hr_rs = np.zeros(hr_vol_scaled.shape)
    non_zero_coo_50 = np.array([np.where(hr_vol_scaled > 50)[x] for x in range(3)]).T    
    non_zero_coo = np.array([np.where(hr_vol_scaled > 10)[x] for x in range(3)]).T
    hr_rs[non_zero_coo[:, 0], non_zero_coo[:, 1], non_zero_coo[:, 2]] = \
        hr_vol_scaled[non_zero_coo[:, 0], non_zero_coo[:, 1], non_zero_coo[:, 2]]
    
    # scale labels matrix (by type value vote)
    hr_label_scaled = np.zeros((subj_segm.shape[0], subj_segm.shape[1], subj_segm.shape[2]))
    count_matrix = np.zeros((subj_segm.shape[0], subj_segm.shape[1], subj_segm.shape[2], 100))
    count_matrix[:] = np.nan
    for x in range(hr_segm.shape[0]):
        for y in range(hr_segm.shape[1]):
            for z in range(hr_segm.shape[2]):
                target_vox = (scf*(x,y,z)).astype(int)
                ind = np.min(np.where(np.isnan(count_matrix[target_vox[0], target_vox[1], target_vox[2], :])))
                count_matrix[target_vox[0], target_vox[1], target_vox[2], ind] = hr_segm[x, y, z]
    for x in range(subj_segm.shape[0]):
        for y in range(subj_segm.shape[1]):
            for z in range(subj_segm.shape[2]):
                votes = count_matrix[x, y, z, :]
                votes = votes[~np.isnan(votes)]
                hr_label_scaled[x, y, z] = np.bincount(votes.astype(int)).argmax()
    
    # Correct verts by co-registering lower left posterior and upper right anterior corners
    correction_vector_2 = np.mean([np.min(non_zero_coo_50, axis=0) - np.min(rr, axis=0), 
                                   np.max(non_zero_coo_50, axis=0) - np.max(rr, axis=0)], axis=0)
    rr = rr + correction_vector_2
    print('Done.')

    # Register
    print('Fitting... ', end='', flush=True)
    subj_vec = subj_segm
    hr_vec = hr_label_scaled
    
    print('Fitting labels... ', end='', flush=True)
    subj_label_ants = ants.from_numpy(subj_vec.astype(float))
    hr_label_ants = ants.from_numpy(hr_label_scaled.astype(float))
    reg = ants.registration(fixed=subj_label_ants, moving=hr_label_ants, type_of_transform='SyNCC')
    def_hr_label = ants.apply_transforms(fixed=subj_label_ants, moving=hr_label_ants,
                                     transformlist=reg['fwdtransforms'], interpolator='genericLabel')
    vox_dir = {'x' : list(rr[:, 0]), 'y' : list(rr[:, 1]), 'z' : list(rr[:, 2])}
    pts = pd.DataFrame(data=vox_dir)
    rrw_0 = np.array(ants.apply_transforms_to_points( 3, pts, reg['invtransforms']))
    
    print('Fitting contrast... ')
    subj_contrast = subj_contrast/np.max(subj_contrast)
    hr_rs = hr_rs/np.max(hr_rs)
    subj_ants = ants.from_numpy(subj_contrast)
    hr_rs_ants = ants.from_numpy(hr_rs)
    hr_ants = ants.apply_transforms(fixed=subj_ants, moving=hr_rs_ants,
                                     transformlist=reg['fwdtransforms'])
    reg = ants.registration(fixed=subj_ants, moving=hr_ants, type_of_transform='SyNCC')
    vox_dir = {'x' : list(rrw_0[:, 0]), 'y' : list(rrw_0[:, 1]), 'z' : list(rrw_0[:, 2])}
    pts = pd.DataFrame(data=vox_dir)
    rrw_1 = np.array(ants.apply_transforms_to_points( 3, pts, reg['invtransforms']))
    hr_label_final = ants.apply_transforms(fixed=subj_ants, moving=def_hr_label,
                                     transformlist=reg['fwdtransforms'], interpolator='genericLabel')
    
    rr_p = rrw_1+cb_range[0]
    subj_cerb.update({'rr' : rr_p})
    subj_cerb.update({'tris' : tris})
    print('Done.')
    
    if calc_nn:
        print('Calculating normals on deformed surface...', end='', flush=True)
        (nn_def, area, area_list, nan_vertices) = evaler.calculate_normals(rr_p, tris, print_info=False)
        subj_cerb.update({'nn' : nn_def})
        subj_cerb.update({'nan_nn' : nan_vertices})
        print('Done.')
    
    # Visualize results as sagittal (x=const) cross-sections
    if plot:
        fig, ax = plot_sagittal(subj, title='Warped points in subj vol', rr=rr_p, tris=tris)
    
    if print_fs:
        print('Saving cerebellar surface as fs files...')
        rr_def = rr_p.copy()
        for x in range(3): rr_def[:, x] = rr_p[:, x]
        print_fs_surf(rr_def, tris, data_dir + subject + '_cerb_cxw.fs', mirror)
        print('Saved to ' + data_dir + subject + '_cerb_cxw.fs')
        
    return subj_cerb


def setup_full_source_space(subject, subjects_dir, cerb_dir, cerb_subsampling='sparse', spacing='oct6',
                            plot_cerebellum=False, debug_mode=False,):
    """Sets up a full surface source space where the first element in the list 
    is the combined cerebral hemishperic source space and the second element
    is the cerebellar source space.
    
    Parameters
    ----------
    subject : str
        Subject name.
    subjects_dir : str
        Subjects directory.
    cerb_dir : str
        Path to cerebellum folder.
    plot_cerebellum : Boolean
        If True, will plot sagittal cross-sectional plots of the cerebellar
        source space superposed on subject MR data.
    spacing : str
        The spacing to use for cortex. Can be ``'ico#'`` for a recursively subdivided
        icosahedron, ``'oct#'`` for a recursively subdivided octahedron,
        or ``'all'`` for all points.
    cerb_subsampling : 'full' | 'sparse' | 'dense'
        The spacing to use for the cerebellum. Can be either full, sparse or dense.

        
    Returns
    -------
    src_whole: list
        List containing two source space elements: the cerebral cortex and the 
        cerebellar cortex.
    
    """
    import mne
#    from evaler import join_source_spaces

    assert cerb_subsampling in ['full', 'sparse', 'dense'], "cerb_subsampling must be either \'full\', \'sparse\' or \'dense\'"
    src_cort = mne.setup_source_space(subject=subject, subjects_dir=subjects_dir, spacing=spacing, add_dist=False)
    if spacing == 'all':
        src_cort[0]['use_tris'] = src_cort[0]['tris']
        src_cort[1]['use_tris'] = src_cort[1]['tris']
    cerb_subj_data = setup_cerebellum_source_space(subjects_dir, subject, cerb_dir, calc_nn=True, cerebellum_subsampling=cerb_subsampling,
                                                   print_fs=True, plot=plot_cerebellum, mirror=False, post_process=True, debug_mode=debug_mode)
    cb_data = pickle.load(open(cerb_dir+'data/cerebellum_geo', 'rb'))
    rr = mne.read_surface(cerb_dir + 'data/' + subject + '_cerb_cxw.fs')[0]/1000
    src_whole = src_cort.copy() 
    hemi_src = join_source_spaces(src_cort)
    src_whole[0] = hemi_src
    src_whole[1]['rr'] = rr
    src_whole[1]['tris'] = cerb_subj_data['tris']
    src_whole[1]['nn'] = cerb_subj_data['nn']
    src_whole[1]['ntri'] = src_whole[1]['tris'].shape[0]
    src_whole[1]['use_tris'] = cerb_subj_data['tris']
    in_use = np.ones(rr.shape[0]).astype(int)
    in_use[cerb_subj_data['nan_nn']] = 0
#    in_use = np.zeros(rr.shape[0])
#    in_use[cb_data['dw_data'][cerb_spacing]] = 1
    src_whole[1]['inuse'] = in_use
    if cerb_subsampling == 'full':
        src_whole[1]['nuse'] = int(np.sum(src_whole[1]['inuse']))
    else:
        src_whole[1]['nuse'] = int(np.sum(src_whole[1]['inuse']))
    src_whole[1]['vertno'] = np.nonzero(src_whole[1]['inuse'])[0]
    src_whole[1]['np'] = src_whole[1]['rr'].shape[0]
    
    return src_whole

=======
>>>>>>> e811129f3e2984f64fe39234dca353de1905fce1
def calculate_dice(vol, ground_truth):
    vol = vol.astype('float')
    ground_truth = ground_truth.astype('float')
    vol_zero = np.where((vol==0))
    vol[vol_zero[0], vol_zero[1], vol_zero[2]] = np.nan
    vol_zero = np.where((ground_truth==0))
    ground_truth[vol_zero[0], vol_zero[1], vol_zero[2]] = np.nan
    dice = 2 * np.sum(vol == ground_truth)/(np.sum(~np.isnan(vol)) + np.sum(~np.isnan(ground_truth)))
    if np.isnan(dice):
        dice = 0
    return dice

def calculate_dice_ind(vol, ground_truth, lob_val):
    vol_inds = np.where(vol == lob_val)
    vol = np.zeros(vol.shape)
    vol[vol_inds] = lob_val
    ground_truth_inds = np.where(ground_truth == lob_val)
    ground_truth = np.zeros(ground_truth.shape)
    ground_truth[ground_truth_inds] = lob_val
    dice = calculate_dice(vol, ground_truth)
    return dice

def rr_to_labels(rr, cb_data, subj_labels):
    vert_to_vox = np.rint(rr).astype(int)
    recon_vert_labels = subj_labels[vert_to_vox[:, 0], vert_to_vox[:, 1], vert_to_vox[:, 2]]
    neighbor_iterations = 4
    while (recon_vert_labels == 0).any():
        zero_verts = np.where(recon_vert_labels == 0)[0]
        recon_vert_labels_temp = recon_vert_labels.copy()
        for c, vert in enumerate(zero_verts):
            neighbors = cb_data['vert_to_neighbor'][vert]
            all_neighbors = neighbors.copy()
            for k in range(neighbor_iterations):
                for neighbor in all_neighbors:
                    neighbors = np.concatenate((cb_data['vert_to_neighbor'][neighbor], neighbors))
                neighbors = np.unique(neighbors)
                all_neighbors = neighbors
            counts = np.bincount(recon_vert_labels[neighbors])
            type_val = np.argmax(counts)
            if type_val == 0 and len(counts) > 1:
                type_val = np.argmax(counts[1:len(counts)])+1
            recon_vert_labels_temp[vert] = type_val
            print(str(c/len(zero_verts)*100)+' % complete', end='\r', flush=True)
        recon_vert_labels = recon_vert_labels_temp
        print('\n verts left : '+str(len((zero_verts)))+'\n')
    return recon_vert_labels

def compute_segmentation_dice(segmentations, ground_truths):
    coarse_groups = [np.arange(1,29), [2, 3, 4, 5, 6], [1]] #WM, vermis, cerebellum
    lobe_groups = [[7,8,9],[18, 19, 20],[10,11,12,13],[21,22,23,24],[14,15,16],[25,26,27],[17],[28]] #8 lobes
    vermis_groups = [[2], [3], [4], [5], [6]] # vermis
    lobule_groups = [[7], [18], [8], [19], [9], [20], [10], [21], [11], [22], [12], [23], 
                     [13], [24], [14], [25], [15], [26], [16], [27], [17], [28]] #22 hemispheric lobules
    
    vol_dice_d = {}
    hierarchies = ['Coarse Division', 'Lobes', 'Vermis', 'Hemispheric Lobules']
    groups = [['cerebellum', 'vermis', 'CM'],
               ['lh anterior', 'rh anterior', 'lh post. sup.', 'rh post. sup.',
                'lh post. inf.', 'rh post. inf.', 'lh flocculus', 'rh flocculus'],
                ['vermis lob 6', 'vermis lob 7', 'vermis lob 8',
                 'vermis lob 9', 'vermis lob 10'],
                 ['lh lob 1-3', 'rh lob 1-3', 'lh lob 4', 'rh lob 4', 'lh lob 5',
                  'rh lob 5', 'lh lob 6', 'rh lob 6', 'lh lob 7af', 'rh lob 7af',
                  'lh lob 7at', 'rh lob 7at', 'lh lob 7b', 'rh lob 7b', 'lh lob 8a',
                  'rh lob 8a', 'lh lob 8b', 'rh lob 8b', 'lh lob 9', 'rh lob 9',
                  'lh lob 10', 'rh lob 10']]
    comparing_methods_best_worst = [[[0.85, 0.95], [0.67, 0.89], [0.65, 0.89]],
                                    [[0.7, 0.86], [0.7, 0.87], [0.75, 0.9],
                                     [0.73, 0.9], [0.84, 0.9], [0.82, 0.91],
                                     [0.58, 0.73], [0.61, 0.77]],
                                    [[0.58, 0.79], [0.42, 0.78], [0.63, 0.89],
                                     [0.58, 0.86], [0.73, 0.85]],
                                    [[0, 0.75], [0, 0.63], [0.5, 0.78],
                                     [0.51, 0.75], [0.52, 0.65], [0.5, 0.65],
                                     [0.71, 0.84], [0.73, 0.85], [0.73, 0.92],
                                     [0.7, 0.9], [0.62, 0.8], [0.63, 0.85],
                                     [0.47, 0.6], [0.48, 0.7], [0.67, 0.73],
                                     [0.5, 0.7], [0.71, 0.86], [0.68, 0.82],
                                     [0.73, 0.9], [0.73, 0.9], [0.58, 0.74],
                                     [0.61, 0.73]]]
    hierarchy_dice = {}
    for b, hierarchy in enumerate([coarse_groups, lobe_groups, vermis_groups, lobule_groups]):
        group_dice = {}
        for c, group in enumerate(hierarchy):
            dices = []
            for segmentation, ground_truth in zip(segmentations, ground_truths):
                vol_1 = segmentation.copy()
                vol_2 = ground_truth.copy()
                inds_1 = np.where(np.isin(vol_1, group))
                inds_2 = np.where(np.isin(vol_2, group))
                vol_1 = np.zeros(vol_1.shape)
                vol_2 = np.zeros(vol_2.shape)
                vol_1[inds_1] = c+1
                vol_2[inds_2] = c+1
                dices.append(calculate_dice(vol_1, vol_2))
            group_dice.update({groups[b][c] : dices})   
        hierarchy_dice.update({hierarchies[b] : group_dice})    
    
    print('plotting segmentation performance...')
    fig, axs = plt.subplots(2, 2, figsize=(8, 8), sharey=True)
    plt.ylim([0, 1])
    plt.yticks(.1*np.arange(11))
    mean_performances = [[],[],[],[]]
    for c, hierarchy in enumerate(hierarchies):
        performance_data = hierarchy_dice[hierarchy]
        categories = performance_data.keys()
        axs[int(c/2)][np.mod(c,2)].set_xticks(np.arange(len((categories))))
        for d, category in enumerate(list(categories)):
            axs[int(c/2)][np.mod(c,2)].scatter(x=np.repeat(d,repeats=len(performance_data[category])),
               y=performance_data[category])
            mean_performance = np.mean(performance_data[category])
            mean_performances[c].append(mean_performance)
            axs[int(c/2)][np.mod(c,2)].scatter(x=d, y=mean_performance, marker='X', color='k', s=120)
            axs[int(c/2)][np.mod(c,2)].scatter(x=[d, d], y=comparing_methods_best_worst[c][d], marker='x', color='r', s=60)
        axs[int(c/2)][np.mod(c,2)].set_xticklabels(list(categories), rotation=90)
    #    axs[int(c/2)][np.mod(c,2)].set_xlabel(hierarchy)
        axs[int(c/2)][np.mod(c,2)].title.set_text(hierarchy)
        axs[int(c/2)][np.mod(c,2)].grid('on',linestyle='--', alpha=0.7, axis='y')
    plt.tight_layout()
    print('mean performane (best comparing mean performance): \n'+
          'coarse: '+str(np.mean(mean_performances[0]))+' (0.912) \n'+
          'lobe: '+str(np.mean(mean_performances[1]))+' (0.839) \n'+
          'vermis: '+str(np.mean(mean_performances[2]))+' (0.830) \n'+
          'lobule: '+str(np.mean(mean_performances[3]))+' (0.766) \n')

    return hierarchy_dice

def print_parcellation(rr_labels, rr, cb_data, fname, labels, RH_factor=0.75, el_face=None):
    if labels == 'atlas':
        lh_vals = [13, 33, 43, 53, 63, 77, 78, 79, 83, 84, 99, 103]
        rh_vals = [16, 36, 46, 56, 66, 71, 72, 73, 86, 87, 96, 106]
        hemis = lh_vals + rh_vals
        vermis = [6, 7, 8, 9, 10, 12]
    if labels == 'challenge':
        lh_vals = [33, 43, 53, 63, 73, 74, 75, 83, 84, 93, 103]
        rh_vals = [36, 46, 56, 66, 76, 77, 78, 86, 87, 96, 106]
        hemis = lh_vals + rh_vals
        vermis = [60, 70, 80, 90, 100, 12]

    array_list = []
    # Color and prepare vertices
    for c, src_point in enumerate(rr_labels):
        if src_point == 0:
            color = (0., 0., 0., 1.)
        elif src_point in rh_vals:
            ind = np.where(np.isin(rh_vals, src_point))[0][0]
            ind_sc = ind/10
            if ind_sc < 1.:
                color = plt.cm.tab10(ind_sc)
            else:
                if ind_sc == 1.1:
                    color = (0.2, 0.2, 0.2, 1.)
                else:
                    color = (1., 1., 0., 1.)
        elif src_point in lh_vals:
            ind = np.where(np.isin(lh_vals, src_point))[0][0]
            ind_sc = ind/10
            if ind_sc < 1.:
                color = plt.cm.tab10(ind_sc)
            else:
                if ind_sc == 1.1:
                    color = (0.2, 0.2, 0.2, 1.)
                else:
                    color = (1., 1., 0., 1.)
        elif src_point in vermis:
            ind = np.where(np.isin(vermis, src_point))[0][0]
            color = plt.cm.Set3(ind/6)
        if src_point in rh_vals:
            color = [int(255.99*val*RH_factor) for val in color]
        else:
            color = [int(255.99*val) for val in color]
        data_tup = (rr[c, 0], rr[c, 1], rr[c, 2], color[0], color[1], color[2], 255)
        array_list.append(data_tup)
        print(str(c/4573612*100) + ' % complete', end='\r', flush=True)

    # Prepare faces
    if type(el_face) == type(None):
        el_face = prepare_faces(cb_data['faces'])
    else:
        el_face = prepare_faces(el_face)
    vertex = np.array(array_list,dtype=[('x', 'float64'), ('y', 'float64'), ('z', 'float64'),
                                         ('red', 'int32'), ('green', 'int32'), ('blue', 'int32'), ('alpha', 'int32')])
    el_vert = PlyElement.describe(vertex,'vertex')
    PlyData([el_vert, el_face], text=True).write(fname)
    return el_vert, el_face

def prepare_faces(faces):
    face_list = []
    for c,x in enumerate(faces):
        data_tup = ([x[0], x[1], x[2]], 1., 1., 1.)        
        face_list.append(data_tup)
        print(str(c/9163916*100) + ' % complete', end='\r', flush=True)
    
    faces = np.array(face_list, dtype=[('vertex_indices', 'int32', (3,)),
                                       ('red', 'int32'), ('green', 'int32'), ('blue', 'int32')])
    el_face = PlyElement.describe(faces,'face')
    return el_face

def remove_verts_from_surface(rr, tris, points_to_keep):
    rr_cropped = rr[points_to_keep, :] # Remove the points outside the volume
    tris_new = tris[np.isin(tris, points_to_keep).all(axis=1), :]
    old_to_new = np.zeros(len(rr))
    old_to_new[points_to_keep] = np.arange(len(points_to_keep))
    tris_new = old_to_new[tris_new].astype(int)
    return rr_cropped, tris_new

<<<<<<< HEAD
def get_segmentation(subjects_dir, subject, cmb_path, region_removal_limit=0.2,
                     post_process=True, print_progress=False, debug_mode=False):
    import warnings
    import subprocess
    import ants

    if not subjects_dir[-1] == '/':
        subjects_dir = subjects_dir +'/'

    data_dir = cmb_path + 'data/segm_folder/'
    if not os.path.exists(data_dir):
        os.system('mkdir '+data_dir)

    # Check that all prerequisite programs are ready 
    if not os.system('mri_convert --help >/dev/null 2>&1') == 0:
        warnings.warn('mri_convert not found. FreeSurfer has to be compiled for segmentation to work.')
    if not os.path.exists(subjects_dir+subject+'/mri/orig.mgz'):
        raise FileNotFoundError('Could not locate subject MRI at '+subjects_dir+subject+'/mri/orig.mgz')
    if not os.system('nnUNet_predict --help >/dev/null 2>&1') == 0:
        raise OSError('nnUNet_predict not found. Please make sure nnUNet is installed and its environment activated and try again.')
        
    if os.path.exists(data_dir+subject+'.nii.gz'): # check if segmentation exists
        print('Previous segmentation found on subject '+subject+'. Returning old segmentation.')
        return nib.load(data_dir+subject+'.nii.gz') # If yes, return

    else: # If not, make segmentation with trained nnUnet model
        rel_paths = ['/tmp/', '/tmp/registered/', '/tmp/registered/whole',
                     '/tmp/registered/lh', '/tmp/registered/rh', '/tmp/registered/mask',
                     '/tmp/registered/lh_segmented', '/tmp/registered/rh_segmented',
                     '/tmp/registered/lob_I_IV', '/tmp/registered/lob_I_IV_segmented',
                     '/tmp/registered/mask_divide']
        for dirs in [data_dir+rel_path for rel_path in rel_paths]:
            if not os.path.exists(dirs):
                os.system('mkdir '+dirs)

        # Load brain template to get a common space
        brain_template_nib = nib.load(cmb_path + 'data/brain.nii')
        brain_template = np.asanyarray(brain_template_nib.dataobj)
        brain_template = brain_template/np.max(brain_template)
        template_ants = ants.from_numpy(brain_template)

        # Register
        output_folder = data_dir+'/tmp/'
        orig_fname = subjects_dir+subject+'/mri/brain.mgz'

        subject_mri = nib.load(orig_fname)
        subj_brain = np.asanyarray(subject_mri.dataobj)
        subj_brain = subj_brain/np.max(subj_brain)
        
        # Find registration from subject to common space
        subj_ants = ants.from_numpy(subj_brain)
        
        # Calculate registration 
        reg = ants.registration(fixed=template_ants, moving=subj_ants, type_of_transform='SyNCC')

        # Prepare for masking
        subj_reg_ants = ants.apply_transforms(fixed=template_ants, moving=subj_ants,
                                              transformlist=reg['fwdtransforms'],
                                              interpolator='nearestNeighbor')
        subj_reg = subj_reg_ants.numpy()
        save_nifti_from_3darray(subj_reg, output_folder + 'registered/whole/' + subject + '_0000.nii.gz',
                                affine=brain_template_nib.affine)
        
        # Mask
        os.system('nnUNet_predict -i ' + output_folder + 'registered/whole/ -o ' + output_folder + 
                  'registered/mask/ -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t 001')
        
        # Split into LH and RH using ASEG
        aseg = np.asanyarray(nib.load(subjects_dir + subject + '/mri/aseg.mgz').dataobj).astype('uint8')
        aseg = ants.from_numpy(aseg)
        aseg_reg = ants.apply_transforms(fixed=template_ants, moving=aseg, transformlist=reg['fwdtransforms'],
                                         interpolator='genericLabel').numpy()

        mask = np.asanyarray(nib.load(output_folder + 'registered/mask/' + subject + '.nii.gz').dataobj)
        split_cerebellar_hemis_aseg(aseg_reg, subj_reg, mask, subject, output_folder + 'registered/',
                                    brain_template_nib.affine)
        
        # Predict LH and RH
        os.system('nnUNet_predict -i ' + output_folder + '/registered/lh/ -o ' + output_folder + 
                  '/registered/lh_segmented/ -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t 002')
        os.system('nnUNet_predict -i ' + output_folder + '/registered/rh/ -o ' + output_folder + 
                  '/registered/rh_segmented/ -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t 003')
        
        # Refine lob I-IV into lobs I-III and IV
        pred_nib = nib.load(output_folder + '/registered/lh_segmented/' + subject + '.nii.gz')
        vol = np.asanyarray(pred_nib.dataobj)
        image = np.asanyarray(nib.load(output_folder + '/registered/lh/' + subject + '_0000.nii.gz').dataobj)
        lobI_IV = np.zeros(vol.shape)
        lobI_IV[np.where(vol == 2)] = image[np.where(vol == 2)]
        pred_nib = nib.load(output_folder + '/registered/rh_segmented/' + subject + '.nii.gz')
        vol = np.asanyarray(pred_nib.dataobj)
        image = np.asanyarray(nib.load(output_folder + '/registered/rh/' + subject + '_0000.nii.gz').dataobj)
        lobI_IV[np.where(vol == 2)] = image[np.where(vol == 2)]
        save_nifti_from_3darray(lobI_IV, output_folder + '/registered/lob_I_IV/' + subject + '_0000.nii.gz',
                                rotate=False, affine=pred_nib.affine)
        
        os.system('nnUNet_predict -i ' + output_folder + '/registered/lob_I_IV/ -o ' + output_folder + 
                  '/registered/lob_I_IV_segmented/ -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t 004')
        
        # Correct labels
        old_labels_ant = [1, 2, 3, 4]
        new_labels_ant = [33, 43, 36, 46]
        old_labels_hemi = np.arange(1, 17)
        new_labels_lh = [12, 43, 53, 63, 73, 74, 75, 83, 84, 93, 103, 60, 70, 80, 90, 100]
        new_labels_rh = [12, 46, 56, 66, 76, 77, 78, 86, 87, 96, 106, 60, 70, 80, 90, 100]
        
        # Assemble segmentations into one image
        seg = np.asanyarray(nib.load(output_folder + '/registered/lh_segmented/' + subject + '.nii.gz').dataobj).astype('uint8')
        seg_lh = change_labels(seg, old_labels_hemi, new_labels_lh)
        seg = np.asanyarray(nib.load(output_folder + '/registered/rh_segmented/' + subject + '.nii.gz').dataobj).astype('uint8')
        seg_rh = change_labels(seg, old_labels_hemi, new_labels_rh)
        seg = np.asanyarray(nib.load(output_folder + '/registered/lob_I_IV_segmented/' + subject + '.nii.gz').dataobj).astype('uint8')
        seg_ant = change_labels(seg, old_labels_ant, new_labels_ant)
        seg_complete = np.zeros(seg.shape)
        seg_complete[np.nonzero(seg_lh)] = seg_lh[np.nonzero(seg_lh)]
        seg_complete[np.nonzero(seg_rh)] = seg_rh[np.nonzero(seg_rh)]
        seg_complete[np.nonzero(seg_ant)] = seg_ant[np.nonzero(seg_ant)]
        seg_ants = ants.from_numpy(seg_complete)
    
        # Go back to subject space
        seg_reg = ants.apply_transforms(fixed=template_ants, moving=seg_ants, transformlist=reg['invtransforms'],
                                         interpolator='genericLabel').numpy()
        
        save_nifti_from_3darray(seg_reg, data_dir + subject + '.nii.gz',
                                rotate=False, affine=subject_mri.affine)

        if not debug_mode:
            for rel_path in rel_paths:
                os.system('rm '+data_dir+rel_path+'/*.nii.gz >/dev/null 2>&1') # Clean up the tmp folder
                os.system('rm '+data_dir+rel_path+'/plans.pkl >/dev/null 2>&1') # Clean up the tmp folder
                os.system('rm '+data_dir+rel_path+'/postprocessing.json >/dev/null 2>&1') # Clean up the tmp folder
        return nib.load(data_dir+subject+'.nii.gz')

def split_cerebellar_hemis_aseg(aseg, brain, mask, subject, output_folder, affine):
    mask_org = mask.copy()
    if not aseg.shape == mask.shape:
        pads = ((np.array(aseg.shape)-np.array(mask.shape))/2).astype(int)
        mask_aligned = np.zeros(aseg.shape)
        mask_aligned[pads[0]:aseg.shape[0]-pads[0], pads[1]:aseg.shape[1]-pads[1],
                     pads[2]:aseg.shape[3]-pads[2]] = mask
        mask = np.array(np.nonzero(mask_aligned)).T
    else:
        mask = np.array(np.nonzero(mask)).T

    lh = np.where(np.isin(aseg, [7, 8]))
    rh = np.where(np.isin(aseg, [46, 47]))
    lh_rh_vol = np.zeros(aseg.shape).astype(int)
    lh_rh_vol[lh] = 1
    lh_rh_vol[rh] = 2
    aseg_cerb = np.concatenate((np.array(lh).T, np.array(rh).T), axis=0)
    aseg_ints = np.dot(aseg_cerb, np.array([1, 256, 256**2]))
    mask_ints = np.dot(mask, np.array([1, 256, 256**2]))
    unsigned_voxels = mask[~(np.isin(mask_ints, aseg_ints))]
    neighbors = np.array([[[[x, y, z] for x in np.arange(-1, 2)] for y in np.arange(-1, 2)] for z in np.arange(-1, 2)]).reshape(27,3)
    
    while len(unsigned_voxels)>0:
        assigned = np.zeros(len(unsigned_voxels))
        type_vals = []
        for c, vox in enumerate(unsigned_voxels):
            all_neighbors = neighbors+vox
            all_neighbors = all_neighbors[np.concatenate((all_neighbors < np.array(lh_rh_vol.shape), all_neighbors > np.array([-1, -1, -1])), axis=1).all(axis=1)]
            val_neighbors = lh_rh_vol[all_neighbors[:, 0], all_neighbors[:, 1], all_neighbors[:, 2]]
            val_neighbors = val_neighbors[~(val_neighbors == 0)] # Remove background
            if len(val_neighbors) > 0:
                counts = np.bincount(val_neighbors)
                type_val = np.argmax(counts)
                type_vals.append(type_val)
                assigned[c] = 1
        vox_to_assign = unsigned_voxels[np.nonzero(assigned)]
        lh_rh_vol[vox_to_assign[:,0], vox_to_assign[:,1], vox_to_assign[:,2]] = type_vals
        unsigned_voxels = unsigned_voxels[np.where(assigned==0)]

    final_split = np.zeros(lh_rh_vol.shape)
    final_split[np.nonzero(mask_org)] = lh_rh_vol[np.nonzero(mask_org)]
    lh_split = np.zeros(brain.shape)
    lh_split[np.where(final_split == 1)] = brain[np.where(final_split == 1)]
    rh_split = np.zeros(brain.shape)
    rh_split[np.where(final_split == 2)] = brain[np.where(final_split == 2)]
    mask = np.zeros(brain.shape)#.astype(int)
    mask[np.where(final_split == 2)] = 2
    mask[np.where(final_split == 1)] = 1

    save_nifti_from_3darray(mask, output_folder+'/mask_divide/'+subject+'_mask_lh_rh.nii.gz', rotate=False, affine=affine)
    save_nifti_from_3darray(lh_split, output_folder+'/lh/'+subject+'_0000.nii.gz', rotate=False, affine=affine)
    save_nifti_from_3darray(rh_split, output_folder+'/rh/'+subject+'_0000.nii.gz', rotate=False, affine=affine)
    
    return


=======
>>>>>>> e811129f3e2984f64fe39234dca353de1905fce1
def mask_cerb(subjects_dir, subject, vol, hemi='both', pad=0):
    aseg = np.asanyarray(nib.load(subjects_dir+subject+'/mri/'+'/aseg.mgz').dataobj)
    if hemi == 'both':
        aseg_inds = [7, 8, 46, 47]
    elif hemi == 'lh':
        aseg_inds = [7, 8]
    elif hemi == 'rh':
        aseg_inds = [46, 47]
    cerb_coords = np.array(np.where(np.isin(aseg, aseg_inds))).T
    cerb_mask = np.zeros(vol.shape)
    cerb_mask[cerb_coords[:, 0], cerb_coords[:, 1], cerb_coords[:, 2]] = \
            vol[cerb_coords[:, 0], cerb_coords[:, 1], cerb_coords[:, 2]]
    cb_range = np.concatenate((np.min(cerb_coords, axis=0), np.max(cerb_coords, axis=0)))
    cerb_mask = cerb_mask[cb_range[0]-pad:cb_range[3]+pad, :, :][:, cb_range[1]-pad:cb_range[4]+pad, :][:, :, cb_range[2]-pad:cb_range[5]+pad]
    return cerb_mask

def print_cerebellum(subjects_dir, subject, fname, hemi='both', pad=0, convert_to_coords = False, crop=False):
    orig_nib = nib.load(subjects_dir+subject+'/mri/'+'/orig.mgz')
    orig = np.asanyarray(orig_nib.dataobj)
    cerebellum = mask_cerb(subjects_dir, subject, orig, hemi=hemi, pad=pad)
    save_nifti_from_3darray(cerebellum, fname+'.nii.gz', rotate=False, affine=orig_nib.affine)
    if type(convert_to_coords) == str:
        os.system('mri_convert --out_orientation '+convert_to_coords+' '+fname+'.nii.gz '+fname+'.nii.gz')
    return cerebellum

def create_label_verts(labels, fwd):
    label_verts = {}
    num = 0
    for label in labels:
        if label.hemi == 'lh':
            hemi_ind = 0
            vert_offset = 0
        if label.hemi == 'rh':
            hemi_ind = 1     
            vert_offset = fwd['src'][0]['nuse']
        verts_lab = label.vertices
        verts_in_src_space = verts_lab[np.isin(verts_lab,fwd['src'][hemi_ind]['vertno'])]
        inds = np.where(np.in1d(fwd['src'][hemi_ind]['vertno'],verts_in_src_space))[0]+vert_offset
        if len(inds) == 0:
            warnings.warn(label.name + ' label has no active source.')
            num = num+1
        label_verts.update({label.name : inds})    
    return label_verts

